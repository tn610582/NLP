{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NMT_en_man.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"UT26eD3M3xmm","colab_type":"code","outputId":"df4f9037-fb3f-49f3-f301-dcf931094927","executionInfo":{"status":"ok","timestamp":1555407235562,"user_tz":180,"elapsed":17735,"user":{"displayName":"Zhiran Wang","photoUrl":"https://lh6.googleusercontent.com/-gve7KvYPjME/AAAAAAAAAAI/AAAAAAAAAmA/IUdQa6KS8Q0/s64/photo.jpg","userId":"05042564609337020530"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"cell_type":"code","source":["#mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"GiwoDz9knB9_","colab_type":"code","outputId":"b1bed561-5aa5-4131-e4be-4dc0eb5fb122","executionInfo":{"status":"ok","timestamp":1555407239284,"user_tz":180,"elapsed":1304,"user":{"displayName":"Zhiran Wang","photoUrl":"https://lh6.googleusercontent.com/-gve7KvYPjME/AAAAAAAAAAI/AAAAAAAAAmA/IUdQa6KS8Q0/s64/photo.jpg","userId":"05042564609337020530"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["gdrive\tsample_data\n"],"name":"stdout"}]},{"metadata":{"id":"b8u5cxxeuXWx","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YcTKmKsVuw3z","colab_type":"code","colab":{}},"cell_type":"code","source":["#data padding and batching\n","\n","MAX_LEN = 50 #maximum words in one line\n","SOS_ID = 1 #ID of the start of line\n","\n","#read language data from file\n","#convert to tensorflow datasets\n","def Makedataset(file_path):\n","  dataset = tf.data.TextLineDataset(file_path)\n","  #convert dataset to a single dimension vector \n","  dataset = dataset.map(lambda string: tf.string_split([string]).values)\n","  #convert the data stream from string to integer\n","  dataset = dataset.map(lambda string: tf.string_to_number(string, tf.int32))\n","  #get the number of words in each line and add the number to the dataset\n","  dataset = dataset.map(lambda x: (x, tf.size(x)))\n","  return dataset\n","\n","\n","#add padding and batching to the source dataset and target dataset\n","def MakeSrcTrgDataset(src_path, trg_path, batch_size):\n","  #read the source data and target data to tf datasets\n","  src_data = Makedataset(src_path)\n","  trg_data = Makedataset(trg_path)\n","  #combine two dataset to one dataset\n","  # using zip() \n","  # data[0][0] : source sentence\n","  # data[0][1] : source sentence length\n","  # data[1][0] : target sentence\n","  # data[1][1] : target sentence length\n","  dataset = tf.data.Dataset.zip((src_data, trg_data))\n","  \n","  #dataset filtering\n","  #delete empty sentence or end only with <eos>\n","  #as well as very long sentences (length > 50 words)\n","  #source: https://www.tensorflow.org/api_docs/python/tf/data/Dataset \n","  def FilterLength(src_tuple, trg_tuple):\n","    ((src_input, src_len), (trg_input, trg_len)) = (src_tuple, trg_tuple)\n","    \n","    src_len_ok = tf.logical_and(\n","        tf.greater(src_len, 1), tf.less_equal(src_len, MAX_LEN))\n","    trg_len_ok = tf.logical_and(\n","        tf.greater(trg_len, 1), tf.less_equal(trg_len, MAX_LEN))\n","    return tf.logical_and(src_len_ok, trg_len_ok)\n","  dataset = dataset.filter(FilterLength) \n","  \n","  #we need data format as input to the encoder as \"<sos> a b c\"\n","  #and uotput format as \"a b c <eos>\"\n","  #chang the dataset format as \"a b c <eos>\"\n","  def MakeTrgInput(src_tuple, trg_tuple):\n","    ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)\n","    trg_input = tf.concat([[SOS_ID], trg_label[:-1]], axis=0)\n","    return ((src_input, src_len), (trg_input, trg_label, trg_len))\n","    \n","  dataset = dataset.map(MakeTrgInput)\n","  \n","  #random shufful prevent overfitting\n","  dataset = dataset.shuffle(10000)\n","  \n","  #define padding shape and size\n","  padded_shape = (\n","    (tf.TensorShape([None]), #source sentence lenth unknown\n","    tf.TensorShape([])),     #length is integer\n","    (tf.TensorShape([None]), #target sentence (decoder input) length unknown\n","    tf.TensorShape([None]),  #target sentence (decoder output) length unknown\n","    tf.TensorShape([])))     #target sentence length is integer\n","  \n","  #use tensorflow padded_batch function to batching the data\n","  batched_dataset = dataset.padded_batch(batch_size, padded_shape)\n","  return batched_dataset"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h_E8JY77x4sz","colab_type":"code","outputId":"1570e1fd-1b06-4870-8b3e-49991fb3d177","executionInfo":{"status":"error","timestamp":1555407090358,"user_tz":180,"elapsed":612,"user":{"displayName":"Zhiran Wang","photoUrl":"https://lh6.googleusercontent.com/-gve7KvYPjME/AAAAAAAAAAI/AAAAAAAAAmA/IUdQa6KS8Q0/s64/photo.jpg","userId":"05042564609337020530"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"cell_type":"code","source":["#test functions\n","src_data = Makedataset(train_data)\n","iterator = src_data.make_one_shot_iterator().get_next()\n","\n","dataset = MakeSrcTrgDataset(train_data, target_data, 20)\n","iterator2 = src_data.make_one_shot_iterator().get_next()\n","\n","with tf.Session() as session:\n","  print(session.run(iterator))  \n","  print(session.run(iterator2))  \n","  print(session.run(iterator2))  \n","\n","  \n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-3254dec3bb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msrc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMakedataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMakeSrcTrgDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miterator2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}]},{"metadata":{"id":"2n8bQg_iy9KO","colab_type":"code","colab":{}},"cell_type":"code","source":["#variables:\n","\n","SRC_TRAIN_DATA = \"/content/gdrive/My Drive/nlp_project/en-zh_data/train_en.txt\" #english training data file\n","TRG_TRAIN_DATA = \"/content/gdrive/My Drive/nlp_project/en-zh_data/train_zh.txt\" #mandarine training data file\n","\n","CHECKPOINT_PATH = \"/content/gdrive/My Drive/nlp_project/en-zh_data/seq2seq_ckpt\" #checkpoint path to save trained network\n","\n","HIDDEN_SIZE = 1024              #LSTM hidden layer size\n","NUM_LAYERS = 2                  #number of LSTM RNN layer\n","SRC_VOCAB_SIZE = 10000          #english vocabulary size\n","TRG_VOCAB_SIZE = 5000           #mandarine vocabulary size\n","BATCH_SIZE = 100                #training batch size\n","NUM_EPOCH = 100                 #number of epoch\n","KEEP_PROB = 0.8                 #probability of dropout nodes\n","MAX_GRAD_NORM = 5               #control gradient limit\n","SHARE_EMB_AND_SOFTMAX = True    #share parameters between softmax layer and emmbeding layer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qq9hxJNwulXz","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","#define translation model\n","class NMTModel(object):\n","  #initialize variables needed in the model\n","  def __init__(self):\n","    #define encoder and decoder's LSTM structure\n","    self.enc_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])\n","    self.dec_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])\n","    #define word embedding for source and target language\n","    self.src_embedding = tf.get_variable(\"src_emb\", [SRC_VOCAB_SIZE, HIDDEN_SIZE])\n","    self.trg_embedding = tf.get_variable(\"trg_emb\", [TRG_VOCAB_SIZE, HIDDEN_SIZE])\n","    \n","    #define softmax layer variables\n","    if SHARE_EMB_AND_SOFTMAX:\n","      self.softmax_weight = tf.transpose(self.trg_embedding)\n","    else:\n","      self.softmax_bias = tf.get_variable(\"weight\", [HIDDED_SIZE, TRG_VOCAB_SIZE])\n","    self.softmax_bias = tf.get_variable(\"softmax_bias\", [TRG_VOCAB_SIZE])\n","    \n","    # defin src_input, src_size, trg_input, trg_label, trg_size\n","    # from MakeSrcTrgDataset \n","  def forward(self, src_input, src_size, trg_input, trg_label, trg_size):\n","      batch_size = tf.shape(src_input)[0]\n","\n","      #word embedding\n","      src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)\n","      trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)\n","\n","      #dropout。\n","      src_emb = tf.nn.dropout(src_emb, KEEP_PROB)\n","      trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)\n","\n","      # construct encoder\n","      with tf.variable_scope(\"encoder\"):\n","          enc_outputs, enc_state = tf.nn.dynamic_rnn(\n","              self.enc_cell, src_emb, src_size, dtype=tf.float32)\n","\n","\n","      # construct decoder\n","      with tf.variable_scope(\"decoder\"):\n","          dec_outputs, _ = tf.nn.dynamic_rnn(\n","              self.dec_cell, trg_emb, trg_size, initial_state=enc_state)\n","\n","      # use log perplexity as loss function\n","      output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])\n","      logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias\n","      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","          labels=tf.reshape(trg_label, [-1]), logits=logits)\n","\n","      # calculation of average cost\n","      label_weights = tf.sequence_mask(\n","          trg_size, maxlen=tf.shape(trg_label)[1], dtype=tf.float32)\n","      label_weights = tf.reshape(label_weights, [-1])\n","      cost = tf.reduce_sum(loss * label_weights)\n","      cost_per_token = cost / tf.reduce_sum(label_weights)\n","\n","      # back propergation of variables\n","      trainable_variables = tf.trainable_variables()\n","\n","      # defin optimization and training steps\n","      grads = tf.gradients(cost / tf.to_float(batch_size),\n","                           trainable_variables)\n","      grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)\n","      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n","      train_op = optimizer.apply_gradients(\n","          zip(grads, trainable_variables))\n","      return cost_per_token, train_op"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gRu5w-Hk1GX0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3SWitLgvSBeL","colab_type":"code","outputId":"29864550-4ef5-4b26-9ce9-bc38e46fc6c4","colab":{"base_uri":"https://localhost:8080/","height":335},"executionInfo":{"status":"error","timestamp":1555407248979,"user_tz":180,"elapsed":305,"user":{"displayName":"Zhiran Wang","photoUrl":"https://lh6.googleusercontent.com/-gve7KvYPjME/AAAAAAAAAAI/AAAAAAAAAmA/IUdQa6KS8Q0/s64/photo.jpg","userId":"05042564609337020530"}}},"cell_type":"code","source":["# train model in epoch\n","def run_epoch(session, cost_op, train_op, saver, step):\n","    # single epoch。\n","    # repeats until train all the data from the dataset\n","    while True:\n","        try:\n","            # run train_op with loss\n","            cost, _ = session.run([cost_op, train_op])\n","            if step % 1 == 0:\n","                print(\"After %d steps, per token cost is %.3f\" % (step, cost))\n","            # save to checkpoint in every 200 steps\n","            if step % 200 == 0:\n","                saver.save(session, CHECKPOINT_PATH, global_step=step)\n","            step += 1\n","        except tf.errors.OutOfRangeError:\n","            break\n","    return step\n","def main():\n","    #reset tensorflow graph\n","    tf.reset_default_graph() \n","    # defin initialization variables\n","    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n","\n","    # defin training nmt model\n","    with tf.variable_scope(\"nmt_model\", reuse=None, \n","                           initializer=initializer):\n","        train_model = NMTModel()\n","  \n","    # defin input data\n","    data = MakeSrcTrgDataset(SRC_TRAIN_DATA, TRG_TRAIN_DATA, BATCH_SIZE)\n","    iterator = data.make_initializable_iterator()\n","    (src, src_size), (trg_input, trg_label, trg_size) = iterator.get_next()\n"," \n","    # Define a forward calculation graph. The input data is supplied to the forward function in tensor form.\n","    cost_op, train_op = train_model.forward(src, src_size, trg_input,\n","                                            trg_label, trg_size)\n","   \n","  # train model。\n","    my_ckpt= \"/content/gdrive/My Drive/nlp_project/en-zh_data/seq2seq_ckpt-800.data-00000-of-00001\" #english training data file\n","    saver = tf.train.Saver()\n","    step = 0\n","    with tf.Session() as sess:\n","        tf.global_variables_initializer().run()\n","        for i in range(NUM_EPOCH):\n","            print(\"In iteration: %d\" % (i + 1))\n","            sess.run(iterator.initializer)\n","            step = run_epoch(sess, cost_op, train_op, saver, step)\n","if __name__ == \"__main__\":\n","    main() "],"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-20fcebc44fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-20fcebc44fc2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# defin input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMakeSrcTrgDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_TRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG_TRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MakeSrcTrgDataset' is not defined"]}]},{"metadata":{"id":"Do9b36XN2CqF","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import codecs\n","import sys"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ozTV90Wm1Knb","colab_type":"code","colab":{}},"cell_type":"code","source":["# checkpoint path \n","CHECKPOINT_PATH = \"/content/gdrive/My Drive/nlp_project/en-zh_data/attention_seq2seq_ckpt-61600\" #checkpoint path to save trained network\n","\n","# hyper parameters\n","HIDDEN_SIZE = 1024                        \n","DECODER_LAYERS  = 2                       \n","SRC_VOCAB_SIZE = 10000                   \n","TRG_VOCAB_SIZE = 5000                    \n","SHARE_EMB_AND_SOFTMAX = True          \n","\n","# Vocabulary files\n","SRC_VOCAB = \"/content/gdrive/My Drive/nlp_project/en-zh_data/train_en.vocab\" \n","TRG_VOCAB = \"/content/gdrive/My Drive/nlp_project/en-zh_data/train_zh.vocab\"\n","\n","# id of <sos> and <eos>\n","SOS_ID = 1\n","EOS_ID = 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gnt5Ha0E1Cw8","colab_type":"code","colab":{}},"cell_type":"code","source":["#define translation model\n","class NMTModel(object):\n","    #initialize variables needed in the model\n","    def __init__(self):\n","      #define encoder and decoder's LSTM structure\n","        self.enc_cell_fw = tf.keras.layers.LSTMCell(HIDDEN_SIZE)\n","        self.enc_cell_bw = tf.keras.layers.LSTMCell(HIDDEN_SIZE)\n","        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(\n","          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) \n","           for _ in range(DECODER_LAYERS)])\n","\n","        # word embedding   \n","        self.src_embedding = tf.get_variable(\n","            \"src_emb\", [SRC_VOCAB_SIZE, HIDDEN_SIZE])\n","        self.trg_embedding = tf.get_variable(\n","            \"trg_emb\", [TRG_VOCAB_SIZE, HIDDEN_SIZE])\n","\n","        # softmax layer\n","        if SHARE_EMB_AND_SOFTMAX:\n","            self.softmax_weight = tf.transpose(self.trg_embedding)\n","        else:\n","            self.softmax_weight = tf.get_variable(\n","               \"weight\", [HIDDEN_SIZE, TRG_VOCAB_SIZE])\n","        self.softmax_bias = tf.get_variable(\n","            \"softmax_bias\", [TRG_VOCAB_SIZE])\n","\n","    def inference(self, src_input):\n","        # conver input to a batch size of 1\n","        src_size = tf.convert_to_tensor([len(src_input)], dtype=tf.int32)\n","        src_input = tf.convert_to_tensor([src_input], dtype=tf.int32)\n","        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)\n","\n","        with tf.variable_scope(\"encoder\"):\n","            # initialize encoder\n","            enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n","                self.enc_cell_fw, self.enc_cell_bw, src_emb, src_size, \n","                dtype=tf.float32)\n","            enc_outputs = tf.concat([enc_outputs[0], enc_outputs[1]], -1)    \n","        \n","        with tf.variable_scope(\"decoder\"):\n","            # initializa decoder\n","            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n","                HIDDEN_SIZE, enc_outputs,\n","                memory_sequence_length=src_size)\n","\n","            attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n","                self.dec_cell, attention_mechanism,\n","                attention_layer_size=HIDDEN_SIZE)\n","   \n","        # defin maximum decode step to prevent looping\n","        MAX_DEC_LEN=100\n","\n","        with tf.variable_scope(\"decoder/rnn/attention_wrapper\"):\n","            # tensor arra to store sentence\n","            init_array = tf.TensorArray(dtype=tf.int32, size=0,\n","                dynamic_size=True, clear_after_read=False)\n","            # decoder input start with <sos>\n","            init_array = init_array.write(0, SOS_ID)\n","            init_loop_var = (\n","                attention_cell.zero_state(batch_size=1, dtype=tf.float32),\n","                init_array, 0)\n","\n","            # use tf while loop untill shows <eos>\n","            def continue_loop_condition(state, trg_ids, step):\n","                return tf.reduce_all(tf.logical_and(\n","                    tf.not_equal(trg_ids.read(step), EOS_ID),\n","                    tf.less(step, MAX_DEC_LEN-1)))\n","\n","            def loop_body(state, trg_ids, step):\n","                # read last step word embedding vector\n","                trg_input = [trg_ids.read(step)]\n","                trg_emb = tf.nn.embedding_lookup(self.trg_embedding,\n","                                                 trg_input)\n","                # call attention_cell to calculate a step backwords\n","                dec_outputs, next_state = attention_cell.call(\n","                    state=state, inputs=trg_emb)\n","                # get the output\n","                output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])\n","                logits = (tf.matmul(output, self.softmax_weight)\n","                          + self.softmax_bias)\n","                next_id = tf.argmax(logits, axis=1, output_type=tf.int32)\n","                # conver source id to target language id\n","                trg_ids = trg_ids.write(step+1, next_id[0])\n","                return next_state, trg_ids, step+1\n","\n","            # run tf.while_loop\n","            state, trg_ids, step = tf.while_loop(\n","                continue_loop_condition, loop_body, init_loop_var)\n","            return trg_ids.stack()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v_m1g80f1VpC","colab_type":"code","outputId":"01322bb1-c7ca-4aaa-e033-7ce0f79412d9","executionInfo":{"status":"ok","timestamp":1555408157868,"user_tz":180,"elapsed":2520,"user":{"displayName":"Zhiran Wang","photoUrl":"https://lh6.googleusercontent.com/-gve7KvYPjME/AAAAAAAAAAI/AAAAAAAAAmA/IUdQa6KS8Q0/s64/photo.jpg","userId":"05042564609337020530"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"cell_type":"code","source":["# clear curren graph\n","tf.reset_default_graph() \n","def main():\n","    # defin NMT model\n","    with tf.variable_scope(\"nmt_model\", reuse=None):\n","        model = NMTModel()\n","\n","    # Testing sentence\n","    #test_en_text = \"Today I 'm going to talk about a project in language translation . <eos>\"\n","    #test_en_text = \"The weather is good .  <eos>\"\n","    #test_en_text = \"write report takes a lot of time .  <eos>\"\n","    #test_en_text = \"The burger week is end . <eos>\"\n","    \n","    \n","    \n","    \n","    \n","    \n","    test_en_text = \"I can finish this report with some help<eos>\"\n","    # convert ID to words\n","    with codecs.open(SRC_VOCAB, \"r\", \"utf-8\") as f_vocab:\n","        src_vocab = [w.strip() for w in f_vocab.readlines()]\n","        src_id_dict = dict((src_vocab[x], x) for x in range(len(src_vocab)))\n","    test_en_ids = [(src_id_dict[token] if token in src_id_dict else src_id_dict['<unk>'])\n","                   for token in test_en_text.split()]\n","    #print(test_en_ids)\n","\n","    # construct decoder\n","    output_op = model.inference(test_en_ids)\n","    sess = tf.Session()\n","    saver = tf.train.Saver()\n","    saver.restore(sess, CHECKPOINT_PATH)\n","\n","    # read translate result\n","    output_ids = sess.run(output_op)\n","    \n","  \n","    \n","    # convert target ID to target sentence\n","    with codecs.open(TRG_VOCAB, \"r\", \"utf-8\") as f_vocab:\n","        trg_vocab = [w.strip() for w in f_vocab.readlines()]\n","    output_text = ''.join([trg_vocab[x] for x in output_ids])\n","    \n","    # print result\n","    print('Source language:',test_en_text)\n","    print('Source number sequence:',test_en_ids)\n","    print('Target number sequence:',output_ids)\n","    print('Target language:',output_text.encode('utf8').decode(sys.stdout.encoding))\n","    sess.close()\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":36,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/nlp_project/en-zh_data/attention_seq2seq_ckpt-61600\n","Source language: I can finish this report with some help<eos>\n","Source number sequence: [11, 29, 1858, 19, 1563, 28, 96, 0]\n","Target number sequence: [  1   5  29  20 249  73  10  12 305 715   4 205 835  14   9  25  13 129\n","   5   3 585 274   6 585 274  13  25  16 585  89   6 585  89 395 205  73\n","  14   6   2]\n","Target language: <sos>我可以完成这个讲述，结束了一些有关我的报告。报告有些人报道。报道总结成了。<eos>\n"],"name":"stdout"}]},{"metadata":{"id":"sSuS4-81ws-s","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ah2gxouKf6Hy","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}